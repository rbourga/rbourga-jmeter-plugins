= Results Comparator Plugin =
This plugin may help you quantify the performance difference between two test executions.<br>
It computes [https://en.wikipedia.org/wiki/Effect_size#Cohen's_d Cohen's ''d''] of the samplers, which is one of the most popular measures of the effect size.<br>
Cohen's d formula is the difference between the means of the two runs divided by a "pooled standard deviation" of the data.
Once ''d'' calculated, the plugin will describe the magnitude of the difference between the means as suggested by Sawilowsky's rule of thumb.

The plugin will follow the threshold values as initially suggested by Cohen and expanded by Sawilowsky as per the following table:

{| class="wikitable"
|-
! d !! Effect size
|-
| 0 || Similar
|-
| < 0.01 || Negligible
|-
| [0.01-0.19] || Very small
|-
| [0.20-0.49] || Small
|-
| [0.50-0.79] || Medium
|-
| [0.80-1.19] || Large
|-
| [1.20-1.99] || Very large
|-
| >= 2.0 || Huge
|}

== Pass / Fail Test ==
To help in Pass/Fail testing, the plugin lets you specify a threshold that you would not want the Cohen's ''d'' to exceed. Any samplers which have their Cohen's ''d'' greater or equal to this threshold shall be marked as "failed" in the results table.<br>
The default value of the threshold is 1.2, which corresponds to Sawilosky's "Very large" limit. By changing this value, you can be more or less forgiving.

== Setup ==
Download the plugin using the JMeter Plugins Manager.

== Usage via the GUI ==
Add the plugin to a Test Plan via the "Add > Non-Test Elements" menu.<br>
[https://github.com/rbourga/jmeter-plugins-2/blob/main/tools/resultscomparator/src/site/img/wiki/rc_NonTestElement.PNG NonTestElement]

=== Calculation of the difference between the means  ===
Change the default Pass/Fail threshold criteria if you wish and than specify the two JMeter results file you want to compare, i.e., the "Control" and "Variation" files and then hit "Compare".<br>
The plugin will compare the means of the samplers in the Variation file to the ones in the Control file. Comparisons will only take place if there are at least two events per sampler in each file.<br>

To help you gauge the global performance of the two runs, the plugin will also compute Cohen's ''d'' across all averages (i.e., average of averages) and give you an "OVERALL AVERAGE" score.

Below is an example of how results are displayed, with different combinations of samplers in the Control and Variation files:.

[https://github.com/rbourga/jmeter-plugins-2/blob/main/tools/resultscomparator/src/site/img/wiki/rc_Compare.PNG Compare]

=== Saving the Comparison results ===
You can export the results table into a file should you wish to add the magnitude of the mean differences in your test report.
By clicking on the "Save" button, the plugin will copy the comparison results into a csv file in the same folder of the Variation file which you can then import into your final test report.

[https://github.com/rbourga/jmeter-plugins-2/blob/main/tools/resultscomparator/src/site/img/wiki/rc_Save.PNG Save]

== Running the plugin from CLI in CI/CD ==
You can also run the plugin from the command line interface (CLI) in a continuous integration and delivery pipelines.<br>
Execute the following command:

<code>java -jar cmdrunner-x.y.z.jar --tool ResultsComparator --thld {myThreshold} --ctrl-file {myControlFile} --var-file {fileToCompare}</code>

where:<br>
* cmdrunner-x.y.z.jar = cmdrunner present in your jmeter/lib directory;
* thld = your Cohen's d threshold value for the pass/fail test;
* ctrl-file = path to your baseline file;
* var-file = path to the file you want to compare

The plugin will return the number of failed samplers in the exit code, allowing to fail the build step in the CI/CD tool if the build process of the tool uses a non zero exit code as a failed indicator.<br>
The plugin will also save the results comparison table in an html report that can be included into the build results of your CI/CD tool.

